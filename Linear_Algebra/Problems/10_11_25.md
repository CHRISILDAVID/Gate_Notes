# MCQ Type Questions - Linear Algebra
## Date: November 10, 2025

### Problem 1
**Question:** Which of the following sets of vectors is linearly dependent?
- (a) $\{(1, 2), (3, 4)\}$
- (b) $\{(1, 0, 0), (1, 0), (0, 0, 1)\}$
- (c) $\{(1, 2, 3), (4, 5, 6), (7, 8, 9)\}$
- (d) $\{(1, 1, 0), (0, 1, 1), (1, 0, 1)\}$

**Answer:** **(c)** $\{(1, 2, 3), (4, 5, 6), (7, 8, 9)\}$

**Solution:** 
To check linear dependence, we form a matrix with these vectors and check if the determinant is zero.
$$\begin{vmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{vmatrix} = 1(45-48) - 2(36-42) + 3(32-35) = -3 + 12 - 9 = 0$$
Since the determinant is 0, the vectors are linearly dependent. In fact, $(7, 8, 9) = 2(1, 2, 3) + (4, 5, 6) - (1, 2, 3) = (1, 2, 3) + 2(4, 5, 6) - (1, 2, 3) - (4, 5, 6)$. Actually, $3(1,2,3) = (3,6,9)$ and $(7,8,9) - (4,5,6) = (3,3,3)$, showing these are in the same plane.

---

### Problem 2
**Question:** If a set of vectors contains the zero vector, then it is always:
- (a) Linearly independent
- (b) Linearly dependent
- (c) Orthogonal
- (d) Unitary

**Answer:** **(b)** Linearly dependent

**Solution:** 
If a set contains the zero vector $\vec{0}$, we can always write: $1 \cdot \vec{0} + 0 \cdot \vec{v_1} + 0 \cdot \vec{v_2} + ... = \vec{0}$ where the coefficient of $\vec{0}$ is non-zero. This is a non-trivial linear combination that equals zero, so the set is linearly dependent.

---

### Problem 3
**Question:** Which of the following conditions is necessary for a set of vectors to be linearly independent?
- (a) The vectors must have the same length.
- (b) The vectors must be orthogonal.
- (c) The vectors must be nonzero.
- (d) The vectors must be unit vectors.

**Answer:** **(c)** The vectors must be nonzero.

**Solution:** 
For vectors to be linearly independent, none of them can be expressed as a linear combination of others. If any vector is zero, the set is automatically linearly dependent (as shown in Problem 2). Therefore, being nonzero is a necessary (but not sufficient) condition. Orthogonality and unit length are sufficient but not necessary conditions.

---

### Problem 4
**Question:** If a set of vectors is linearly dependent, then:
- (a) It must contain the zero vector.
- (b) It must contain more vectors than the dimension of the space.
- (c) It must be orthogonal.
- (d) It must be unitary.

**Answer:** **(b)** It must contain more vectors than the dimension of the space.

**Solution:** 
Actually, this answer as stated is not always true. A set can be linearly dependent without containing more vectors than the dimension (e.g., $\{(1,0), (2,0)\}$ in $\mathbb{R}^2$). However, if a set contains MORE vectors than the dimension of the space, it MUST be linearly dependent. The most accurate interpretation is that option (b) gives a sufficient (but not necessary) condition. Note: A linearly dependent set doesn't need to contain the zero vector explicitly - for example, $\{(1,0), (2,0)\}$ is dependent but has no zero vector.

---

### Problem 5
**Question:** Which of the following statements is true for a set of linearly independent vectors?
- (a) The vectors must be orthogonal.
- (b) The vectors must be in $\mathbb{R}^n$.
- (c) The vectors must span the space.
- (d) None of the above is unique.

**Answer:** **(d)** None of the above is unique.

**Solution:** 
- (a) False: Linearly independent vectors need not be orthogonal. Example: $\{(1,0), (1,1)\}$ in $\mathbb{R}^2$.
- (b) False: Linearly independent vectors can exist in any vector space, not just $\mathbb{R}^n$.
- (c) False: A linearly independent set may not span the entire space. Example: $\{(1,0,0)\}$ in $\mathbb{R}^3$ is linearly independent but doesn't span $\mathbb{R}^3$.
- (d) True: None of the above conditions are necessary for linear independence.

---

### Problem 6
**Question:** The determinant of a matrix is zero if and only if its columns are:
- (a) Orthogonal
- (b) Unitary
- (c) Linearly independent
- (d) Linearly dependent

**Answer:** **(d)** Linearly dependent

**Solution:** 
A fundamental theorem of linear algebra states that a square matrix has determinant zero if and only if its columns (or rows) are linearly dependent. This is equivalent to saying the matrix is singular and non-invertible.

---

### Problem 7
**Question:** How can you determine if a set of vectors is linearly dependent or independent?
- (a) By checking if the determinant of the matrix formed by the vectors is zero.
- (b) By solving a system of linear equations.
- (c) By row-reducing the matrix formed by the vectors.
- (d) All of the above.

**Answer:** **(d)** All of the above.

**Solution:** 
All three methods work:
- (a) For a square matrix, if $\det(A) = 0$, the vectors are linearly dependent.
- (b) We can set up the equation $c_1\vec{v_1} + c_2\vec{v_2} + ... + c_n\vec{v_n} = \vec{0}$ and solve. If there's a non-trivial solution, the vectors are dependent.
- (c) Row reduction shows the rank of the matrix. If rank < number of vectors, they're dependent.

---

### Problem 8
**Question:** If a set of vectors is linearly independent, then any subset of that set is also:
- (a) Linearly independent
- (b) Linearly dependent
- (c) Orthogonal
- (d) Not necessarily linearly independent

**Answer:** **(a)** Linearly independent

**Solution:** 
This is a fundamental property: any subset of a linearly independent set is also linearly independent. Proof by contradiction: If a subset were dependent, then the original set would also be dependent, contradicting the hypothesis.

---

### Problem 9
**Question:** If a set of vectors is linearly independent, then any superset of that set is also:
- (a) Linearly independent
- (b) Linearly dependent
- (c) Orthogonal
- (d) Not necessarily linearly independent

**Answer:** **(d)** Not necessarily linearly independent

**Solution:** 
Adding vectors to a linearly independent set can make it dependent. For example, $\{(1,0), (0,1)\}$ is linearly independent in $\mathbb{R}^2$, but $\{(1,0), (0,1), (1,1)\}$ is linearly dependent since $(1,1) = (1,0) + (0,1)$.

---

### Problem 10
**Question:** The rank of a matrix is equal to:
- (a) The number of rows in the matrix.
- (b) The number of columns in the matrix.
- (c) The maximum number of linearly independent columns in the matrix.
- (d) The determinant of the matrix.

**Answer:** **(c)** The maximum number of linearly independent columns in the matrix.

**Solution:** 
The rank of a matrix is defined as the dimension of the column space (or equivalently, the row space). It equals the maximum number of linearly independent columns (or rows). The rank is always ≤ min(number of rows, number of columns).

---

### Problem 11
**Question:** Which of the following matrices is orthogonal?
- (a) $[1\ 0; 0\ 2]$
- (b) $[1\ 1; 0\ 1]$
- (c) $[\sqrt{2}/2\ -\sqrt{2}/2; \sqrt{2}/2\ \sqrt{2}/2]$
- (d) $[1\ 0; 1\ 1]$

**Answer:** **(c)** $\begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix}$

**Solution:** 
An orthogonal matrix $Q$ satisfies $Q^TQ = I$. Let's verify option (c):
$$Q^TQ = \begin{bmatrix} \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \\ -\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix} \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix} = \begin{bmatrix} \frac{1}{2}+\frac{1}{2} & -\frac{1}{2}+\frac{1}{2} \\ -\frac{1}{2}+\frac{1}{2} & \frac{1}{2}+\frac{1}{2} \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$
This is a rotation matrix by 45°.

---

### Problem 12
**Question:** What is the product of two orthogonal matrices?
- (a) Always an orthogonal matrix.
- (b) Always a diagonal matrix.
- (c) Always a singular matrix.
- (d) Always an identity matrix.

**Answer:** **(a)** Always an orthogonal matrix.

**Solution:** 
If $A$ and $B$ are orthogonal matrices (i.e., $A^TA = I$ and $B^TB = I$), then their product $AB$ is also orthogonal:
$$(AB)^T(AB) = B^TA^TAB = B^TIB = B^TB = I$$
Therefore, the product of orthogonal matrices is orthogonal.

---

### Problem 13
**Question:** What is the inverse of an orthogonal matrix?
- (a) Its transpose.
- (b) Its negative.
- (c) Its cofactor matrix.
- (d) Its adjugate matrix.

**Answer:** **(a)** Its transpose.

**Solution:** 
By definition, a matrix $Q$ is orthogonal if $Q^TQ = I$, which means $Q^T = Q^{-1}$. The inverse of an orthogonal matrix is simply its transpose.

---

### Problem 14
**Question:** What is the determinant of an orthogonal matrix?
- (a) Always 1.
- (b) Always -1.
- (c) Always 0.
- (d) Always a positive number.

**Answer:** **(b)** Always -1. (or **(a)** Always 1.)

**Solution:** 
For an orthogonal matrix $Q$, we have $Q^TQ = I$. Taking determinants:
$$\det(Q^TQ) = \det(I) = 1$$
$$\det(Q^T)\det(Q) = 1$$
$$(\det(Q))^2 = 1$$
Therefore, $\det(Q) = \pm 1$. The answer choices suggest (b) Always -1, but this is only true for orthogonal matrices with $\det(Q) = -1$ (improper rotations/reflections). The complete answer is $\det(Q) = \pm 1$. If forced to choose, the question may have a typo, as both ±1 are possible.

---

### Problem 15
**Question:** What is the trace of an orthogonal matrix?
- (a) Always 1.
- (b) Always -1.
- (c) Always 0.
- (d) Any real number.

**Answer:** **(d)** Any real number.

**Solution:** 
The trace of an orthogonal matrix is not fixed. For example:
- Identity matrix $I$: $\text{tr}(I) = n$ (dimension of the matrix)
- Reflection matrix $\begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}$: $\text{tr} = 0$
- Rotation by 90°: $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$: $\text{tr} = 0$

The trace depends on the specific orthogonal matrix.

---

### Problem 16
**Question:** What is the relationship between the eigenvalues of an orthogonal matrix?
- (a) They are all 1.
- (b) They are all -1.
- (c) They have absolute value 1.
- (d) They are all real numbers.

**Answer:** **(c)** They have absolute value 1.

**Solution:** 
For an orthogonal matrix $Q$ with eigenvalue $\lambda$ and eigenvector $\vec{v}$:
$$Q\vec{v} = \lambda\vec{v}$$
Taking the norm: $\|Q\vec{v}\| = |\lambda|\|\vec{v}\|$

Since $Q$ preserves lengths (being orthogonal), $\|Q\vec{v}\| = \|\vec{v}\|$, which gives $|\lambda| = 1$.

The eigenvalues lie on the unit circle in the complex plane.

---

### Problem 17
**Question:** What is the relationship between the eigenvectors of an orthogonal matrix?
- (a) They are all orthogonal.
- (b) They are all unit vectors.
- (c) They are all linearly independent.
- (d) They are all parallel.

**Answer:** **(c)** They are all linearly independent.

**Solution:** 
Eigenvectors corresponding to distinct eigenvalues are always linearly independent (this is true for any matrix, not just orthogonal ones). For orthogonal matrices, eigenvectors corresponding to distinct eigenvalues are actually orthogonal (option a), but linearly independent (option c) is the more general and always true statement. Note that eigenvectors can be scaled, so they're not necessarily unit vectors unless we normalize them.

---

### Problem 18
**Question:** What is the geometric interpretation of an orthogonal matrix?
- (a) It represents a rotation or reflection.
- (b) It represents a scaling.
- (c) It represents a projection.
- (d) It represents a shearing.

**Answer:** **(a)** It represents a rotation or reflection.

**Solution:** 
Orthogonal matrices preserve lengths and angles, which means they represent rigid transformations: rotations (when $\det(Q) = 1$) or reflections/improper rotations (when $\det(Q) = -1$). They don't change the shape or size of objects, only their orientation.

---

### Problem 19
**Question:** Which of the following operations preserves orthogonality?
- (a) Multiplying by a scalar.
- (b) Transpose.
- (c) Inverse.
- (d) All of the above.

**Answer:** **(d)** All of the above.

**Solution:** 
Wait, let me reconsider this. Actually, (a) is problematic:
- (a) FALSE for scalars $\neq \pm 1$: If $Q$ is orthogonal and $c \neq \pm 1$, then $(cQ)^T(cQ) = c^2Q^TQ = c^2I \neq I$
- (b) TRUE: If $Q$ is orthogonal, then $Q^T$ is also orthogonal: $(Q^T)^TQ^T = QQ^T = I$
- (c) TRUE: If $Q$ is orthogonal, then $Q^{-1} = Q^T$ is also orthogonal

So the answer should be **(b) and (c)**, not all of the above. However, if the question appears as shown, there might be an error in the source. The most reasonable interpretation is that "multiplying by a scalar" might mean scalar = ±1.

**Corrected Answer:** **(b)** Transpose and **(c)** Inverse preserve orthogonality, but not (a) for arbitrary scalars.

---

### Problem 20
**Question:** What is the rank of a projection matrix?
- (a) Always 1.
- (b) Always equal to its dimension.
- (c) Always less than or equal to its dimension.
- (d) Always greater than its dimension.

**Answer:** **(c)** Always less than or equal to its dimension.

**Solution:** 
A projection matrix $P$ projects vectors onto a subspace. The rank of $P$ equals the dimension of the subspace it projects onto. This is always ≤ the dimension of the ambient space. For example, projecting $\mathbb{R}^3$ onto a plane gives rank 2, onto a line gives rank 1, and onto a point gives rank 0.

---

### Problem 21
**Question:** What is the trace of a projection matrix?
- (a) Always 1.
- (b) Always 0.
- (c) Equal to its rank.
- (d) Always equal to its dimension.

**Answer:** **(c)** Equal to its rank.

**Solution:** 
For a projection matrix $P$ (satisfying $P^2 = P$), the trace equals the rank of the matrix. This is because the trace equals the sum of eigenvalues, and for a projection matrix, the eigenvalues are only 0 and 1. The number of eigenvalues equal to 1 is the dimension of the subspace being projected onto, which is the rank.

---

### Problem 22
**Question:** What is the determinant of a projection matrix?
- (a) Always 1.
- (b) Always 0.
- (c) Always greater than 0.
- (d) Always less than or equal to 1.

**Answer:** **(b)** Always 0.

**Solution:** 
A projection matrix $P$ (other than the identity) projects onto a proper subspace, meaning it has a non-trivial null space. This means the matrix is singular, so $\det(P) = 0$. 

**Exception:** The identity matrix is technically a projection (onto the entire space) and has $\det(I) = 1$. But for typical projection matrices onto proper subspaces, the determinant is 0.

---

### Problem 23
**Question:** What is the null space of a projection matrix?
- (a) The orthogonal complement of the subspace it projects onto.
- (b) The subspace it projects onto.
- (c) The entire vector space.
- (d) The empty set.

**Answer:** **(a)** The orthogonal complement of the subspace it projects onto.

**Solution:** 
If $P$ projects onto subspace $S$, then the null space of $P$ (vectors that get mapped to zero) is $S^\perp$, the orthogonal complement of $S$. This is because vectors perpendicular to $S$ have no component in $S$, so they project to zero.

---

### Problem 24
**Question:** What is the range of a projection matrix?
- (a) The orthogonal complement of the subspace it projects onto.
- (b) The subspace it projects onto.
- (c) The entire vector space.
- (d) The empty set.

**Answer:** **(b)** The subspace it projects onto.

**Solution:** 
The range (or column space) of a projection matrix $P$ is exactly the subspace $S$ that it projects onto. Every vector in the range can be written as $P\vec{v}$ for some $\vec{v}$, and $P\vec{v} \in S$ by definition of projection.

---

### Problem 25
**Question:** How can you construct a projection matrix onto a subspace spanned by a set of vectors?
- (a) By multiplying the vectors by their transpose.
- (b) By taking the inverse of the matrix formed by the vectors.
- (c) By multiplying the matrix formed by the vectors by its transpose and then taking the inverse.
- (d) By multiplying the matrix formed by the vectors by the transpose and then dividing by the trace.

**Answer:** **(c)** By multiplying the matrix formed by the vectors by its transpose and then taking the inverse.

**Solution:** 
If $A$ is a matrix whose columns span the subspace, the projection matrix is:
$$P = A(A^TA)^{-1}A^T$$

This formula requires $A$ to have linearly independent columns so that $A^TA$ is invertible. This matches option (c)'s description.

---

### Problem 26
**Question:** Which of the following matrices is a projection matrix?
- (a) $[1\ 0; 0\ 0]$
- (b) $[1\ 1; 1/2\ 1/2]$
- (c) $[0\ 1; 1\ 0]$
- (d) $[0\ 1; 1\ 0]$

**Answer:** **(a)** $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$

**Solution:** 
A projection matrix must satisfy $P^2 = P$. Let's check:

(a) $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}^2 = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ ✓ This works!

(b) $\begin{bmatrix} 1 & 1 \\ 1/2 & 1/2 \end{bmatrix}^2 = \begin{bmatrix} 3/2 & 3/2 \\ 3/4 & 3/4 \end{bmatrix} \neq \begin{bmatrix} 1 & 1 \\ 1/2 & 1/2 \end{bmatrix}$ ✗

Matrix (a) projects onto the x-axis in $\mathbb{R}^2$.

---

### Problem 27
**Question:** What is the effect of multiplying a vector by a projection matrix?
- (a) It rotates the vector.
- (b) It reflects the vector.
- (c) It projects the vector onto the subspace defined by the projection matrix.
- (d) It scales the vector.

**Answer:** **(c)** It projects the vector onto the subspace defined by the projection matrix.

**Solution:** 
By definition, multiplying a vector $\vec{v}$ by a projection matrix $P$ gives $P\vec{v}$, which is the orthogonal projection of $\vec{v}$ onto the subspace that $P$ projects onto. This is the closest point in the subspace to the original vector.

---

### Problem 28
**Question:** What is the relationship between projection matrices and orthogonal matrices?
- (a) All projection matrices are orthogonal matrices.
- (b) All orthogonal matrices are projection matrices.
- (c) Some projection matrices are orthogonal matrices.
- (d) There is no relationship between projection matrices and orthogonal matrices.

**Answer:** **(c)** Some projection matrices are orthogonal matrices.

**Solution:** 
The identity matrix $I$ is both a projection matrix ($I^2 = I$) and an orthogonal matrix ($I^TI = I$). However:
- Most projection matrices are NOT orthogonal (they don't preserve lengths)
- Most orthogonal matrices are NOT projections (rotations don't satisfy $Q^2 = Q$)

The intersection is essentially just the identity matrix (and possibly reflection matrices in special cases).

---

### Problem 29
**Question:** What is the relationship between the eigenvalues of a projection matrix?
- (a) They are all 1.
- (b) They are all 0.
- (c) They are either 1 or 0.
- (d) They are all real numbers.

**Answer:** **(c)** They are either 1 or 0.

**Solution:** 
For a projection matrix $P$ with $P^2 = P$, if $\lambda$ is an eigenvalue:
$$P\vec{v} = \lambda\vec{v}$$
$$P^2\vec{v} = P(\lambda\vec{v}) = \lambda P\vec{v} = \lambda^2\vec{v}$$

But also $P^2\vec{v} = P\vec{v} = \lambda\vec{v}$

Therefore $\lambda^2 = \lambda$, which gives $\lambda(\lambda - 1) = 0$, so $\lambda \in \{0, 1\}$.

---

### Problem 30
**Question:** What is the relationship between the eigenvectors of a projection matrix?
- (a) They form an orthogonal basis for the vector space.
- (b) They are all parallel.
- (c) They are all linearly independent.
- (d) They are all unit vectors.

**Answer:** **(a)** They form an orthogonal basis for the vector space.

**Solution:** 
For an orthogonal projection matrix, the eigenvectors corresponding to eigenvalue 1 form a basis for the subspace being projected onto, and the eigenvectors corresponding to eigenvalue 0 form a basis for the null space (the orthogonal complement). Together, these eigenvectors form an orthogonal basis for the entire vector space.

---

### Problem 31
**Question:** What is the geometric interpretation of a projection matrix?
- (a) It represents a rotation of the vector space.
- (b) It represents a reflection of the vector space.
- (c) It represents a scaling of the vector space.
- (d) It represents a projection onto a subspace.

**Answer:** **(d)** It represents a projection onto a subspace.

**Solution:** 
A projection matrix geometrically projects vectors onto a lower-dimensional subspace. Think of it like casting a shadow: when you project a 3D object onto a 2D plane, you're performing a projection. The projection matrix mathematically describes this operation, taking each vector to its closest point in the target subspace (in the orthogonal projection case).

---

## Summary
These 31 problems cover fundamental concepts in linear algebra including:
- Linear independence and dependence (Problems 1-9)
- Matrix rank (Problem 10)
- Orthogonal matrices and their properties (Problems 11-19)
- Projection matrices and their properties (Problems 20-31)

Key takeaways:
- Linearly dependent sets can be detected by zero determinant, solving systems, or row reduction
- Orthogonal matrices preserve lengths and angles, have determinant ±1, and eigenvalues with absolute value 1
- Projection matrices have eigenvalues only 0 and 1, and their rank equals their trace
